pd.read_csv(filepath_or_buffer, sep=’, ‘, delimiter=None, header=’infer’, names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression=’infer’, thousands=None, decimal=b’.’, lineterminator=None, quotechar=’”’, quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=None, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, skip_footer=0, doublequote=True, delim_whitespace=False, as_recarray=None, compact_ints=None, use_unsigned=None, low_memory=True, buffer_lines=None, memory_map=False, float_precision=None)

惊喜不惊喜，刺激不刺激？下面就详细解读下，个别参数的意义。（我个人认为有用的）

filepath_or_buffer : 路径 URL 可以是http, ftp, s3, 和 file.

sep: 指定分割符，默认是’,’C引擎不能自动检测分隔符，但Python解析引擎可以

delimiter: 同sep

delimiter_whitespace: True or False 默认False, 用空格作为分隔符等价于spe=’\s+’如果该参数被调用，则delimite不会起作用

header: 指定第几行作为列名(忽略注解行)，如果没有指定列名，默认header=0; 如果指定了列名header=None

names 指定列名，如果文件中不包含header的行，应该显性表示header=None

index_col: 默认为None 用列名作为DataFrame的行标签，如果给出序列，则使用MultiIndex。如果读取某文件,该文件每行末尾都有带分隔符，考虑使用index_col=False使panadas不用第一列作为行的名称。

不知道你有没有看懂index_col最后一句话。反正我是没看懂，尝试了以后也没啥效果。后来尝试用下面方法来解决上述问题： 
其中test.txt 的内容为： 
1;2;3;4;5; 
6;7;8;2;5; 
注意这里how一定要用all,不然的话，一旦某一列出现na就会把整列数据删掉。

path2='datas/test.txt'
df1 = pd.read_csv(path2, sep=';',low_memory=False,header=None)
df1= df1.dropna(axis=1,how='all')
df1.head() ## 获取前五行数据查看查看
1
2
3
4
usecols： 默认None 可以使用列序列也可以使用列名，如 [0, 1, 2] or [‘foo’, ‘bar’, ‘baz’]

as_recarray：默认False , 将读入的数据按照numpy array的方式存储，0.19.0版本后使用 
pd.read_csv(…).to_records()。 注意，这种方式读入的na数据不是显示na,而是给以个莫名奇妙的值

squeeze: 默认为False, True的情况下返回的类型为Series

prefix:默认为none, 当header =None 或者没有header的时候有效，例如’x’ 列名效果 X0, X1, …

mangle_dupe_cols ：默认为True,重复的列将被指定为’X.0’…’X.N’，而不是’X’…’X’。如果传入False，当列中存在重复名称，则会导致数据被覆盖。

dtype: E.g. {‘a’: np.float64, ‘b’: np.int32} 指定数据类型

engine: {‘c’, ‘python’}, optional 选择读取的引擎目前来说C更快，但是Python的引擎有更多选择的操作

skipinitialspace: 忽略分隔符后的空格,默认false,

skiprows: list-like or integer or callable, default None 忽略某几行或者从开始算起的几行

skipfooter: 从底端算起的几行，不支持C引擎

nrows： int 读取的行数

na_values: 默认None NaN包含哪些情况，默认情况下, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘n/a’, ‘nan’, ‘null’. 都表现为NAN

keep_default_na: 如果na_values被定义,keep_default_na为False那么默认的NAN会被改写。 默认为True

na_filter: 默认为True, 针对没有NA的文件，使用na_filter=false能够提高读取效率

skip_blank_lines 默认为True,跳过blank lines 而且不是定义为NAN

thousands 千分位符号，默认‘，’

decimal 小数点符号，默认‘.’

encoding: 编码方式

memory_map如果为filepath_or_buffer提供了文件路径，则将文件对象直接映射到内存上，并直接从那里访问数据。使用此选项可以提高性能，因为不再有任何I / O开销。

low_memory 默认为True 在块内部处理文件，导致分析时内存使用量降低，但可能数据类型混乱。要确保没有混合类型设置为False，或者使用dtype参数指定类型。请注意，不管怎样，整个文件都读入单个DataFrame中，请使用chunksize或iterator参数以块形式返回数据。 （仅在C语法分析器中有效）